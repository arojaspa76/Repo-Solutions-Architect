# Reference architecture for LLMs

When it comes to large language model (LLM) applications in the cloud, each of the major providers has published **reference architectures** that describe the major components, design patterns, security/governance measures, and typical deployment topologies. Below are the key references:

### • Azure  
- **Article**: “Baseline Microsoft Foundry Chat Reference Architecture”  
  URL: https://learn.microsoft.com/en-us/azure/architecture/ai-ml/architecture/baseline-azure-ai-foundry-chat :contentReference[oaicite:0]{index=0}  
- **Description**: Shows how to build and deploy an enterprise chat application using Azure AI Foundry + Azure OpenAI. Includes UI layer, agent/orchestrator, data stores (for grounding), private network integration, managed identity, and security controls.

### • AWS  
- **Page**: “AWS Reference Architecture Diagrams”  
  URL: https://aws.amazon.com/architecture/reference-architecture-diagrams/ :contentReference[oaicite:1]{index=1}  
- **Description**: A library of diagrams built by AWS that cover generative AI, machine learning, data lakes, and agent-based workloads. While not always specific to LLMs, they serve as high-level reference for AI/ML architecture on AWS.

### • GCP (Google Cloud Platform)  
- **Page**: “AI and machine learning resources | Cloud Architecture Center”  
  URL: https://cloud.google.com/architecture/ai-ml :contentReference[oaicite:2]{index=2}  
- **Description**: Offers generative AI, agentic AI, RAG, ML Ops reference architectures. Includes design guides and reference diagrams on deploying AI/ML workloads at enterprise scale on Google Cloud.

---